{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631c6370",
   "metadata": {},
   "source": [
    "#### 1. A. What is the business objective you are looking to solve for? Specifically. \n",
    "\n",
    " The primary business objective is to identify which customers would benefit the most from US Foods' \n",
    " daily delivery service. This service comes at an increased operational cost, so it's essential to \n",
    " target customers who are not only willing to but will also find substantial value in using this service, \n",
    " thereby increasing their loyalty and lifetime value.\n",
    "\n",
    "#### 1. B. What decisions are you going to inform?  \n",
    "\n",
    " Decisions to be Informed:\n",
    " \n",
    " Identification of customer segments that would most benefit from daily delivery service.\n",
    " Optimal pricing strategies for this enhanced service to ensure profitability.\n",
    " Inventory and supply chain adjustments needed to accommodate daily deliveries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550a095",
   "metadata": {},
   "source": [
    " #### 2. Which variable/s would you select as independent/dependent variables? And why? \n",
    "\n",
    "##### Scenario 1: Predicting Customer Spending with US Foods\n",
    "Dependent Variable: REPORTED_WKLY_SPEND_USF (How much do you spend each week with US Foods)\n",
    "\n",
    "Independent Variables: REPORTED_WKLY_SPEND, SMALL_QTY_RANK, DEL_FLEX_RANK, CUT_TIME_RANK, WKLY_ORDERS, PERC_EB, MENU_TYP_DESC, PYR_SEG_CD, AVG_WKLY_SALES, AVG_WKLY_CASES\n",
    "\n",
    "Why: The goal here is to understand how various factors affect a customer's weekly spending with US Foods specifically. Independent variables are chosen based on their potential to influence the dependent variable.\n",
    "\n",
    "##### Scenario 2: Predicting Importance of Flexible Delivery Schedule\n",
    "Dependent Variable: DEL_FLEX_RANK (How important is having a flexible delivery schedule)\n",
    "\n",
    "Independent Variables: REPORTED_WKLY_SPEND, REPORTED_WKLY_SPEND_USF, WKLY_ORDERS, MENU_TYP_DESC, PYR_SEG_CD\n",
    "\n",
    "Why: Here, we want to understand what types of customers find a flexible delivery schedule to be most important. Therefore, we choose variables that could potentially affect this preference.\n",
    "\n",
    "##### Scenario 3: Predicting Overall Weekly Spending with All Distributors\n",
    "Dependent Variable: REPORTED_WKLY_SPEND (How much do you spend each week with all distributors)\n",
    "\n",
    "Independent Variables: REPORTED_WKLY_SPEND_USF, REPORTED_WKLY_SPEND_COMP, WKLY_ORDERS, PERC_EB, MENU_TYP_DESC, PYR_SEG_CD, AVG_WKLY_SALES, AVG_WKLY_CASES\n",
    "\n",
    "Why: The aim is to predict how much a customer will spend in total each week with all distributors. Independent variables are selected based on their likely impact on total weekly spending.\n",
    "\n",
    "##### Scenario 4: Predicting Spending on Produce\n",
    "Dependent Variable: REPORTED_PRODUCE_SPEND (How much of your purchasing in dollars with all sellers is Produce)\n",
    "\n",
    "Independent Variables: REPORTED_WKLY_SPEND, REPORTED_WKLY_SPEND_USF, MENU_TYP_DESC, PYR_SEG_CD, AVG_WKLY_SALES, AVG_WKLY_CASES\n",
    "\n",
    "Why: The goal is to understand what factors influence spending on produce. Variables related to overall spending and type of restaurant could be key drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431143ea",
   "metadata": {},
   "source": [
    "#### 3. What kind of feature engineering did you (or would you) try on these variables? \n",
    "\n",
    "Aggregation: Create new features that aggregate spends across various suppliers to get a holistic picture.\n",
    "\n",
    "Encoding: One-hot or label encode categorical features like MENU_TYP_DESC.\n",
    "\n",
    "Normalization/Standardization: As the scales of the features might differ, normalizing them could be beneficial.\n",
    "\n",
    "Interaction Terms: Create interaction features that combine two or more features, which might provide additional insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2ac10",
   "metadata": {},
   "source": [
    "#### 4. Choose a model that you believe best fits the dataset. Why did you choose this model? \n",
    "\n",
    "I would initially try a Random Forest Classifier for this task because:\n",
    "\n",
    "It handles a mix of numerical and categorical features well.\n",
    "It is less prone to overfitting compared to decision trees.\n",
    "Feature importance can be easily extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018917f",
   "metadata": {},
   "source": [
    "#### 5. Explain the accuracy of your model. Along with the explanation include any distinction or nuance with results that you want to share with your audience. Provide any recommendations you would suggest for actions to improve the success of this new service. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86b0c1",
   "metadata": {},
   "source": [
    "####  5. Model Accuracy \n",
    "\n",
    "Accuracy: It's essential to look beyond just accuracy; metrics like Precision, Recall, \n",
    "    and F1-score should also be considered, especially if the classes are imbalanced.\n",
    "\n",
    "Nuances: A high accuracy on its own is not sufficient; the model should generalize well to new data. \n",
    "    Cross-validation scores will help in understanding this aspect.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Conduct A/B tests to validate model predictions in a real-world setting.\n",
    "With more data you can set up a feedback loop for continuously improving the model.\n",
    "If the model indicates that only a small, high-value segment is likely to use this service, it might be worth exploring tiered pricing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9684b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries and Load Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Read data from Excel files NOTE: Change file directory path as needed with this dataset\n",
    "df_survey = pd.read_excel(\"C:/Users/Kyle/Documents/US Foods/Survey Data Assessment Final.xlsx\", sheet_name='survey_data_assessment')\n",
    "df_transaction = pd.read_excel(\"C:/Users/Kyle/Documents/US Foods/Survey Data Assessment Final.xlsx\", sheet_name='transactional_data')\n",
    "\n",
    "# Merge or join both datasets based on common identifier if necessary\n",
    "# Assuming 'CUST_NBR' is the common identifier\n",
    "df = pd.merge(df_survey, df_transaction, on='CUST_NBR', how='inner')\n",
    "\n",
    "# Check and handle missing values in df\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    # Handle numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    df[numerical_cols] = df[numerical_cols].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    \n",
    "    # Handle non-numerical columns\n",
    "    non_numerical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    df.dropna(subset=non_numerical_cols, inplace=True)\n",
    "\n",
    "# Check and replace infinite values\n",
    "if np.isinf(df[numerical_cols]).sum().sum() > 0:\n",
    "    df[numerical_cols] = df[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numerical_cols] = df[numerical_cols].apply(lambda x: x.fillna(x.mean()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935cd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "\n",
    "# Assuming 'REPORTED_WKLY_SPEND_USF_DAILY_SERVICE' is your target variable\n",
    "# Making it binary to indicate whether they are a good target for daily service\n",
    "# Here I assume that spending more than 100 is considered as interested in daily service\n",
    "df['target'] = np.where(df['REPORTED_WKLY_SPEND_USF_DAILY_SERVICE'] > 100, 1, 0)\n",
    "\n",
    "# Drop the original 'REPORTED_WKLY_SPEND_USF_DAILY_SERVICE' column\n",
    "df.drop(['REPORTED_WKLY_SPEND_USF_DAILY_SERVICE'], axis=1, inplace=True)\n",
    "\n",
    "# Define features X and target y\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Feature columns to be used\n",
    "numerical_features = ['REPORTED_WKLY_SPEND_USF', 'WKLY_ORDERS', 'PERC_EB', 'AVG_WKLY_SALES']\n",
    "categorical_features = ['MENU_TYP_DESC', 'PYR_SEG_CD']\n",
    "\n",
    "# Create a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3db901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Building\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f47a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9807692307692307\n",
      "Confusion Matrix:\n",
      "[[ 53   4]\n",
      " [  5 406]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92        57\n",
      "           1       0.99      0.99      0.99       411\n",
      "\n",
      "    accuracy                           0.98       468\n",
      "   macro avg       0.95      0.96      0.96       468\n",
      "weighted avg       0.98      0.98      0.98       468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Evaluation\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee2cb3",
   "metadata": {},
   "source": [
    "#### Interpretation of Results\n",
    "\n",
    "Accuracy: The accuracy score is 97.65%, which looks quite good.\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "True Positive: 405 (Class 1 correctly identified as Class 1)\n",
    "\n",
    "True Negative: 52 (Class 0 correctly identified as Class 0)\n",
    "\n",
    "False Positive: 5 (Class 0 incorrectly identified as Class 1)\n",
    "\n",
    "False Negative: 6 (Class 1 incorrectly identified as Class 0)\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "The Precision, Recall, and F1-score for Class 1 are pretty high, close to 99%.\n",
    "For Class 0, the values are lower but still quite good (around 88%-93%).\n",
    "\n",
    "#### Some things that need to be considered. \n",
    "Class Imbalance: The dataset seems to be imbalanced given that Class 1 has many more samples of 405 compared to Class 0 which has 52. This could make the model biased towards Class 1. Going to investigate using techniques like SMOTE, ADASYN, or simply oversampling the minority class to balance the classes.\n",
    "\n",
    "Overfitting: High accuracy could also be a sign of overfitting. Considering using cross-validation to get a better sense of how well the model will generalize to unseen data.\n",
    "\n",
    "Feature Importance: Since I selected using a RandomForestClassifier, I'm considering looking at other feature importances to understand which variables are contributing the most to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e91e32",
   "metadata": {},
   "source": [
    "#### Checking for class imbalance \n",
    "This is crucial for ensuring that your machine learning model performs well not just on the overall dataset, but also on each individual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04039f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: target\n",
      "1    2050\n",
      "0     288\n",
      "Name: count, dtype: int64\n",
      "Class Distribution as Percentage: target\n",
      "1    87.681779\n",
      "0    12.318221\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking the class distribution of the target variable\n",
    "class_distribution = df['target'].value_counts()\n",
    "print(\"Class Distribution:\", class_distribution)\n",
    "\n",
    "# To see the class distribution as a percentage\n",
    "class_distribution_percentage = (df['target'].value_counts(normalize=True) * 100)\n",
    "print(\"Class Distribution as Percentage:\", class_distribution_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d5ddf",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Class 1 (customers spending more than $100 weekly with US Foods and are considered potential targets for daily service): 2050 occurrences, which is 87.68 percent of the dataset\n",
    "\n",
    "Class 0 (customers spending $100 or less): 288 occurrences, which is 12.32% of the dataset.\n",
    "\n",
    "The class distribution indicates that you have a class imbalance problem. Specifically, almost 88% of the samples are of class 1 (\"interested in daily service\"), while only about 12% are of class 0 (\"not interested in daily service\").\n",
    "\n",
    "This imbalance could lead to biased predictions, as the model might be more inclined to predict the majority class. Several techniques can be employed to mitigate this issue: Which I will walk through 2 methods under sampling and over sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418afae",
   "metadata": {},
   "source": [
    "#### Undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effba165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    288\n",
      "0    288\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.target==1]\n",
    "df_minority = df[df.target==0]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=len(df_minority),  # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Display new class counts\n",
    "print(df_downsampled.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6973d9",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "Loss of Data: One of the primary concerns with undersampling is the loss of data. In this case, I reduced the majority class from 2050 instances to 288, which means we're not utilizing a significant portion of available data. This could potentially lead to loss of valuable patterns. \n",
    "\n",
    "This could result in overfitting.\n",
    "\n",
    "Overfitting: Since there is a smaller sample, the model might overfit to this sample and might not generalize well on unseen data.\n",
    "\n",
    "Future Model Evalutation: It's important to evaluate the model on an unbiased dataset. If using the downsampled data for training, ensure that there is a separate, unbiased test set to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133c01f",
   "metadata": {},
   "source": [
    "#### Oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d640a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    2050\n",
      "0    2050\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),  # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "print(df_upsampled.target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e5e7c",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "Increasing the Data: Upsampling results in an increase in the dataset size. While this can help in achieving class balance, there's a possibility of introducing noise and overfitting since the same samples from the minority class can be repeated.\n",
    "\n",
    "Avoids Loss of Data: Unlike undersampling, where data from the majority class is discarded, oversampling retains all original data.\n",
    "\n",
    "Model Evaluation: As with undersampling, if you decide to train a model on the upsampled data, make sure you have an unbiased test set to evaluate its performance. This will ensure you're getting an accurate picture of how the model will perform on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6ae35",
   "metadata": {},
   "source": [
    "#### Balancing the data set. Objective here is to aim for a model that performs well across all classes. So we will be resampling  the data with this code.\n",
    "\n",
    "To put in better context, I am taking the downsampled data because it helps to mitigate the effect of a class imbalance by reducing the majority class to match the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2872b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after resampling: 1.0\n",
      "Confusion Matrix after resampling:\n",
      "[[53  0]\n",
      " [ 0 63]]\n",
      "Classification Report after resampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        53\n",
      "           1       1.00      1.00      1.00        63\n",
      "\n",
      "    accuracy                           1.00       116\n",
      "   macro avg       1.00      1.00      1.00       116\n",
      "weighted avg       1.00      1.00      1.00       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier #RandomForest is versatile and it often performs well for classification tasks \n",
    "                                                    #and also less prone to overfitting\n",
    "from sklearn.pipeline import Pipeline #Selected for easier and robust preprocessing\n",
    "from sklearn.compose import ColumnTransformer #Allows for applying different preprocessing steps to the numeric and \n",
    "                                              #categorical columns in the data set. This is crucial for handling data sets \n",
    "                                              #with both types of variables\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder #StandardScaler is used to scale numeric features\n",
    "                                                                #OneHotEncoder is used to encode categorical variables\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define your transformers for numeric and categorical data\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')  # Added handle_unknown parameter due to unknown category errors\n",
    "\n",
    "# Your actual feature names\n",
    "numeric_features = ['REPORTED_WKLY_SPEND', 'REPORTED_WKLY_SPEND_USF', 'REPORTED_WKLY_SPEND_COMP', 'REPORTED_WKLY_SPEND_COMPETITOR_1',\n",
    "                    'REPORTED_WKLY_SPEND_OTHER_BLD', 'REPORTED_WKLY_SPEND_SPCLTY', 'REPORTED_WKLY_SPEND_CC',\n",
    "                    'REPORTED_WKLY_SPEND_OTHER', 'REPORTED_COP_SPEND', 'REPORTED_PRODUCE_SPEND', 'SMALL_QTY_RANK', \n",
    "                    'DEL_FLEX_RANK', 'CUT_TIME_RANK', 'WKLY_ORDERS', 'PERC_EB', 'AVG_WKLY_SALES', 'AVG_WKLY_CASES']\n",
    "\n",
    "categorical_features = ['DIV_NBR', 'MENU_TYP_DESC', 'PYR_SEG_CD']\n",
    "\n",
    "# Column Transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Assuming df_downsampled or df_upsampled is your new DataFrame after resampling\n",
    "# Define features X and target y\n",
    "X_resampled = df_downsampled.drop('target', axis=1)\n",
    "y_resampled = df_downsampled['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train_resampled, X_test_resampled, y_train_resampled, y_test_resampled = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred_resampled = pipeline.predict(X_test_resampled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy after resampling:\", accuracy_score(y_test_resampled, y_pred_resampled))\n",
    "print(\"Confusion Matrix after resampling:\")\n",
    "print(confusion_matrix(y_test_resampled, y_pred_resampled))\n",
    "print(\"Classification Report after resampling:\")\n",
    "print(classification_report(y_test_resampled, y_pred_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed87245",
   "metadata": {},
   "source": [
    "## Interpretation \n",
    "Accuracy: The model achieved an accuracy of approximately 99.1% on the test set. This indicates that almost all the predictions made by the model on this set were correct.\n",
    "\n",
    "##### Confusion Matrix:\n",
    "\n",
    "True Negative = 51: The model correctly predicted 51 instances of class 0 (no interest in spending).\n",
    "\n",
    "False Positive = 2: The model incorrectly predicted that 2 instances belong to class 1 when they actually belong to class 0.\n",
    "\n",
    "True Positive = 63: The model correctly predicted 63 instances of class 1 (interest in spending).\n",
    "\n",
    "False Negative = 0: The model made no incorrect predictions of class 0 when they actually belong to class 1.\n",
    "\n",
    "##### Classification Report:\n",
    "\n",
    "Precision for Class 0: 1.00 means that every instance predicted as class 0 was actually class 0.\n",
    "\n",
    "Recall for Class 0: 0.96 indicates that the model caught 96% of the actual class 0 instances.\n",
    "\n",
    "Precision for Class 1: 0.97 implies that 97% of the instances predicted as class 1 were correct.\n",
    "\n",
    "Recall for Class 1: 1.00 indicates that the model caught every single instance of class 1.\n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, which is useful when the class distribution is unbalanced. Both classes have F1-scores of 0.98, which is excellent.\n",
    "\n",
    "Macro avg and Weighted avg: These provide the average scores of precision, recall, and F1-score across both classes. A high value in both these averages suggests that the model has performed uniformly well across both classes.\n",
    "\n",
    "##### What does this mean for the model?\n",
    "\n",
    "Improved Model Performance: The balanced dataset seems to have improved the overall performance of the model, especially when considering the minority class (class 0).\n",
    "\n",
    "Generalization: Even though the results on the test set are promising, we should be cautious about how the model might generalize to new, unseen data, especially given the undersampling approach.\n",
    "\n",
    "Comparison with Previous Model: These results appear better compared to the model trained on the imbalanced dataset, especially concerning precision and recall for class 0. The high recall for both classes suggests that the model misses very few instances from both categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a034d675",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "In this step I'm going to validate the performance of the model. Which will involve partioning the original training data set into a 'k' subsets, using 'k-1' of these subsets to train the model, and validating it on the remaining one. This process is done k times, each time with a different subset used as the validation set. The k results can then be averaged to produce a single estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7544b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores for each fold:  [0.88793103 0.97391304 0.99130435 1.         1.        ]\n",
      "Average cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define pipeline as before\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Using the resampled data as before\n",
    "X_resampled = df_downsampled.drop('target', axis=1)\n",
    "y_resampled = df_downsampled['target']\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print each fold and average\n",
    "print(\"Cross-validation scores for each fold: \", cross_val_scores)\n",
    "print(\"Average cross-validation score: {:.2f}\".format(cross_val_scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43916e",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "Cross-validation scores: The scores for the 5 folds vary between approximately 88.79% and 100%. The variability in performance across folds is somewhat expected given the nature of cross-validation where different parts of the dataset are held out as test sets in each iteration.\n",
    "\n",
    "Average Cross-Validation Score: An average score of 97% is very promising. This, combined with your earlier test results, suggests that your model is likely to achieve similar performance on unseen data.\n",
    "\n",
    "Variability: The fact that one fold has an accuracy of about 88.79% while others are above 98% suggests some level of variability in the model's performance. This could indicate sensitivity to particular subsets of the data or potential overfitting.\n",
    "\n",
    "Consistency: The other four scores are notably consistent, ranging from 98.3% to 100%. This is a good sign, suggesting that the model is quite robust to different splits of the data, although the lower score on the first fold warrants further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d161643",
   "metadata": {},
   "source": [
    "#### Investigating the lower score\n",
    "Curious to find out what may have caused the lower score as there could be an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc0431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified instances:\n",
      "Empty DataFrame\n",
      "Columns: [CUST_NBR, REPORTED_WKLY_SPEND, REPORTED_WKLY_SPEND_USF, REPORTED_WKLY_SPEND_COMP, REPORTED_WKLY_SPEND_COMPETITOR_1, REPORTED_WKLY_SPEND_OTHER_BLD, REPORTED_WKLY_SPEND_SPCLTY, REPORTED_WKLY_SPEND_CC, REPORTED_WKLY_SPEND_OTHER, REPORTED_COP_SPEND, REPORTED_PRODUCE_SPEND, SMALL_QTY_RANK, DEL_FLEX_RANK, CUT_TIME_RANK, DIV_NBR, WKLY_ORDERS, PERC_EB, MENU_TYP_DESC, PYR_SEG_CD, AVG_WKLY_SALES, AVG_WKLY_CASES]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n",
      "Class distribution in test set:\n",
      "target\n",
      "0    58\n",
      "1    57\n",
      "Name: count, dtype: int64\n",
      "Feature importances: [1.98822918e-01 3.67367786e-01 3.23042155e-02 9.97878588e-03\n",
      " 6.01405940e-03 9.24020137e-03 5.30497355e-03 6.00367662e-03\n",
      " 5.89077772e-02 3.59657787e-02 1.76967280e-02 1.56720079e-02\n",
      " 1.58812284e-02 5.53099830e-02 3.54088126e-02 1.30682513e-02\n",
      " 2.07322496e-02 4.43055891e-04 1.10802979e-03 8.70854893e-04\n",
      " 6.97341211e-04 1.01864557e-03 5.18137225e-04 5.48493999e-04\n",
      " 2.84994746e-04 1.20811687e-06 1.21645976e-03 2.99508805e-03\n",
      " 9.62875375e-05 2.49982113e-04 8.16701579e-04 1.18314955e-04\n",
      " 7.96100729e-04 5.20657979e-04 5.28217101e-04 8.87391014e-04\n",
      " 4.36256339e-04 1.32275212e-03 1.62103680e-04 3.91662544e-04\n",
      " 5.18637816e-04 6.29678815e-04 5.86553022e-04 2.29972467e-04\n",
      " 1.18021577e-03 2.21820412e-04 1.45670095e-04 1.00399312e-03\n",
      " 1.09469265e-03 1.13083440e-03 1.05234947e-03 4.65460978e-05\n",
      " 1.05672683e-03 1.03669491e-03 1.39727953e-04 0.00000000e+00\n",
      " 1.11898545e-03 3.61370341e-07 1.59182986e-04 0.00000000e+00\n",
      " 1.98171513e-03 3.98395148e-04 3.43920979e-04 1.08177712e-03\n",
      " 4.55244336e-04 6.08659688e-04 2.02949974e-03 1.33302485e-03\n",
      " 9.27345540e-05 1.74933320e-03 0.00000000e+00 2.44176480e-05\n",
      " 2.62814384e-05 6.17104538e-05 1.95093136e-04 7.55910965e-04\n",
      " 3.83766732e-04 6.06274882e-05 8.76196217e-04 6.50534348e-04\n",
      " 8.22700183e-04 1.40310830e-06 1.98403307e-03 8.39446093e-04\n",
      " 1.08346965e-04 0.00000000e+00 9.42356116e-05 9.15595760e-04\n",
      " 2.95252074e-04 4.99703827e-04 1.17617442e-04 4.85465946e-05\n",
      " 1.33295098e-04 1.79892841e-04 8.72966982e-04 1.58224386e-04\n",
      " 1.48822222e-03 0.00000000e+00 3.50768522e-03 2.85079999e-04\n",
      " 1.03194824e-04 1.49817372e-03 9.54261342e-03 1.34504636e-04\n",
      " 3.17858226e-04 5.12555266e-05 8.92684749e-04 0.00000000e+00\n",
      " 0.00000000e+00 8.32492037e-04 3.80355375e-03 9.47489639e-03\n",
      " 4.30147053e-04 6.61357388e-04 2.10456385e-03 9.66707756e-03\n",
      " 2.25361245e-06 1.78855692e-03 3.58740638e-04 1.81417156e-03]\n"
     ]
    }
   ],
   "source": [
    "# Examine the Data Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X_resampled):\n",
    "    X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "    y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
    "    \n",
    "    # Fit and evaluate the model on each fold\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    \n",
    "    # If the score is notably lower, inspect the data\n",
    "    if score < 0.9:\n",
    "        print(\"Inspecting low-performing fold:\")\n",
    "        print(X_test.describe())\n",
    "        \n",
    "# Error Analysis\n",
    "y_pred = pipeline.predict(X_test)\n",
    "misclassified = X_test[y_pred != y_test]\n",
    "print(\"Misclassified instances:\")\n",
    "print(misclassified)\n",
    "\n",
    "# Class Imbalance\n",
    "print(\"Class distribution in test set:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Sensitivity Analysis\n",
    "feature_importance = pipeline.named_steps['classifier'].feature_importances_\n",
    "print(\"Feature importances:\", feature_importance)\n",
    "\n",
    "# Learning Curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    pipeline, X_train, y_train, train_sizes=[50, 80, 110], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef5aed",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "Misclassified Instances:\n",
    "\n",
    "There aren't any misclassified instances in the low-performing fold. This seems contradictory given that the accuracy was less than 0.9 for this fold during cross-validation. It is possible that when I separately ran the training and prediction for this particular fold, the model parameters converged differently and led to a better score.\n",
    "\n",
    "Class Distribution:\n",
    "\n",
    "The test set distribution seems balanced with 58 instances of class '0' and 57 instances of class '1'. This is a good sign, indicating that the resampling worked effectively, and the dataset was evenly split in the folds.\n",
    "\n",
    "Feature Importances:\n",
    "\n",
    "Feature importances give you insight into which features are most influential in predicting the target variable.\n",
    "\n",
    "Some features have zero importance, meaning they do not contribute any information to the model's predictions and could be considered for removal to simplify the model.\n",
    "\n",
    "Some features have high importance, meaning they're vital for the model's decision-making. For instance, the first two features have relatively higher importance values compared to many other features.\n",
    "\n",
    "Learning Curves:\n",
    "\n",
    "Learning curves can be used to understand if the model benefits from more training data or if it's reaching a plateau.\n",
    "\n",
    "Typically, you would plot training scores and validation scores as a function of the number of training examples to interpret how well the model is learning. You can see if the model is overfitting, underfitting, or if it would benefit from more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929333e",
   "metadata": {},
   "source": [
    "#### Adding a Learning Curve (Random Forest Classifer)\n",
    "This would be a good step to evalutate and incorporate for ongoing development as new data comes into the model. It helps to evaluate the performance of a model on a training and validation set. Which can help in understanding if the model is underfitting, overfitting, or if it would benefit from more data. \n",
    "\n",
    "This is also good for scalability. As US Foods collects more data we can look at what indicators are improving the model or if it reaches a plateau. This can help further make future planning decisions.\n",
    "\n",
    "Furthermore, if the learning curve shows that the model can benefit from more data, it can influence business decisions such as data collection or customer engagement strategies. For example, if we discover what positive or negative inputs and outputs that US Foods is doing with their customers or other market players. This can be used to help enhance desireable outcomes like promoting more business in areas that US Foods is doing well in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49dce8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Assuming 'pipeline' is your Pipeline and 'X_resampled', 'y_resampled' are your data\n",
    "plot_learning_curve(pipeline, \"Learning Curve (Random Forest Classifier)\", X_resampled, y_resampled, cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74190d8f",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "The curve starts apart but gets closer as training size increases. This could benefit from having more data so that way it becomes less over fit with more training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353a22a",
   "metadata": {},
   "source": [
    "#### Distributions of Features\n",
    "We can further look at how each feature is represented by comparing the positive and negatives\n",
    "Note: A 0 is negative and a 1 is positive. Also because this is python a 0 is actually 1 for counting purposes. We have 5 features and they go from 0 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2eeb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e2370e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Dash app for feature importance visualization\n",
    "feature_importance_app = dash.Dash(__name__)\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=5,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    random_state=42,\n",
    "    class_sep=2,\n",
    "    n_clusters_per_class=1\n",
    ")\n",
    "\n",
    "# Create DataFrame to hold the data\n",
    "feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Train a Random Forest Classifier model\n",
    "clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Extract feature importances and sort them\n",
    "df_importance = pd.DataFrame({'Feature': feature_names, 'Importance': clf.feature_importances_})\n",
    "df_importance.sort_values('Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Dash app layout\n",
    "feature_importance_app.layout = html.Div([\n",
    "    dcc.Dropdown(\n",
    "        id='feature-dropdown',\n",
    "        options=[{'label': i, 'value': i} for i in feature_names],\n",
    "        value='Feature 0'  # Default value\n",
    "    ),\n",
    "    dcc.Graph(id='feature-importance-plot')\n",
    "])\n",
    "\n",
    "# Callback to update the Graph based on Dropdown selection\n",
    "@feature_importance_app.callback(\n",
    "    Output('feature-importance-plot', 'figure'),\n",
    "    [Input('feature-dropdown', 'value')]\n",
    ")\n",
    "def update_graph(selected_feature):\n",
    "    filtered_df = df[[selected_feature, 'target']]\n",
    "    fig = px.histogram(\n",
    "        filtered_df,\n",
    "        x=selected_feature,\n",
    "        color='target',\n",
    "        title=f'Distribution of {selected_feature} by Target Class',\n",
    "        labels={'target': 'Target Class'}\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# Run the Dash app\n",
    "if __name__ == '__main__':\n",
    "    feature_importance_app.run_server(debug=True, port=8051)  # Running on a different port so the apps don't collide and run over each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fbd45",
   "metadata": {},
   "source": [
    "##### Explaination of the Target and it's class:\n",
    "\n",
    "Target (or Label): This refers to the outcome or result we want our model to predict based on the input features. For instance, in a binary classification problem where you're predicting whether an email is spam or not, the target variable might be a binary indicator: 1 for spam and 0 for not spam.\n",
    "\n",
    "Target Class: In the context of classification, the target variable can have multiple possible values, each of which is referred to as a class. So, the term \"class\" denotes a specific possible value of the target variable.\n",
    "\n",
    "We can see how each of the distributions of each feature with it's target and target class being mostly in alignment. However Feature 4 (the fifth feature) the distribution is out of alignment which is why we investigated the lower score as done above. As stated before with more data because of the bias of the data set it would help to alleviate this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e37ac2",
   "metadata": {},
   "source": [
    "#### Setting up an anomaly detection matrix\n",
    "This would be good to add to a dashboard later for KPIs so that we can take the anomalies and compare the predicted data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9477b49",
   "metadata": {},
   "source": [
    "#### Creating data visualization for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae5d25f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9e7ff0640>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Backend - Anomaly Detection\n",
    "from scipy import stats\n",
    "\n",
    "def find_anomalies(data, threshold=3):\n",
    "    z_scores = stats.zscore(data)\n",
    "    abs_z_scores = abs(z_scores)\n",
    "    anomalies = (abs_z_scores > threshold).all(axis=1)\n",
    "    return anomalies\n",
    "\n",
    "# Anomalies in REPORTED_WKLY_SPEND\n",
    "df_downsampled['anomaly_spend'] = find_anomalies(df_downsampled[['REPORTED_WKLY_SPEND']])\n",
    "\n",
    "# Anomalies in AVG_WKLY_SALES\n",
    "df_downsampled['anomaly_sales'] = find_anomalies(df_downsampled[['AVG_WKLY_SALES']])\n",
    "\n",
    "# Frontend - Visualization using Dash\n",
    "from dash import dash_table, dcc, html, Input, Output\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Filter out normal rows and only show anomalies\n",
    "df_anomalies = df_downsampled[df_downsampled['anomaly_spend'] | df_downsampled['anomaly_sales']]\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1('Anomalies/Alerts Dashboard for US Foods'),\n",
    "\n",
    "    html.Div([\n",
    "        html.H3('Detected Anomalies'),\n",
    "\n",
    "        dash_table.DataTable(\n",
    "            id='table',\n",
    "            columns=[{'name': i, 'id': i} for i in df_anomalies.columns],\n",
    "            data=df_anomalies.to_dict('records'),\n",
    "        )\n",
    "    ], style={'color': 'red'}),\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55451d4",
   "metadata": {},
   "source": [
    "##### Explaination of what's happening here.\n",
    "Backend - Anomaly Detection:\n",
    "\n",
    "This is using the Z-score method for anomaly detection. The Z-score represents how many standard deviations a data point is from the mean. Generally, data points that are far from the mean are considered outliers or anomalies.\n",
    "\n",
    "Applying Anomaly Detection:\n",
    "\n",
    "Anomalies are detected in two columns: REPORTED_WKLY_SPEND and AVG_WKLY_SALES.\n",
    "For each column, I calculated the anomalies and stored the results in new columns (anomaly_spend and anomaly_sales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5966d1",
   "metadata": {},
   "source": [
    "#### Conduct A/B tests to validate model predictions in a real-world setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d19899a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 0.7091819765478322\n",
      "P-value: 0.4784995022341677\n",
      "We fail to reject the null hypothesis: There is no statistical difference in the follow-up metrics between group A and group B.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Check if 'group' column exists, if not, create one\n",
    "if 'group' not in df_downsampled.columns:\n",
    "    df_downsampled['group'] = np.random.choice(['A', 'B'], size=len(df_downsampled))\n",
    "\n",
    "# Making copies of the DataFrame slices to avoid SettingWithCopyWarning\n",
    "group_A = df_downsampled[df_downsampled['group'] == 'A'].copy()\n",
    "group_B = df_downsampled[df_downsampled['group'] == 'B'].copy()\n",
    "\n",
    "# Generating some follow-up metrics (replace this with real data)\n",
    "group_A.loc[:, 'follow_up_metric'] = np.random.rand(len(group_A))\n",
    "group_B.loc[:, 'follow_up_metric'] = np.random.rand(len(group_B))\n",
    "\n",
    "# Conducting a t-test\n",
    "t_stat, p_value = stats.ttest_ind(group_A['follow_up_metric'], group_B['follow_up_metric'])\n",
    "\n",
    "# Print the results\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"We reject the null hypothesis: The follow-up metrics between group A and group B are statistically different.\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis: There is no statistical difference in the follow-up metrics between group A and group B.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6cbcc",
   "metadata": {},
   "source": [
    "##### Explaination of why this is valuable\n",
    "\n",
    "Generation of Follow-Up Metrics:\n",
    "\n",
    "Using a synthetic 'follow_up_metric' for both groups containing random numbers. In real-world scenarios, this metric might be something like user engagement rate, click-through rate, average transaction value, etc.\n",
    "\n",
    "T-Test:\n",
    "\n",
    "I then conducted an independent t-test to check if the mean of 'follow_up_metric' for group A is statistically different from that of group B.\n",
    "\n",
    "The t-test provides two values: the T-statistic and the P-value. The T-statistic indicates how much the groups differ in standard deviation units. The P-value helps determine if the observed data would be very unlikely under the null hypothesis.\n",
    "\n",
    "##### Interpretation\n",
    "T-statistic: The T-statistic is -1.3051013866852241. The negative value indicates that group A's mean is less than group B's mean for the follow-up metric. However, the magnitude of this statistic tells you how much they differ in terms of standard deviations.\n",
    "\n",
    "P-value: The P-value is 0.19238125478360693. This means that there's about a 19.2% probability of observing data as extreme as this sample data, assuming the null hypothesis is true.\n",
    "\n",
    "Alpha: Given an alpha (significance level) of 0.05 (or 5%), you compare the P-value to alpha. Since the P-value is greater than alpha, you fail to reject the null hypothesis. This suggests that the differences you see in the follow-up metrics between group A and group B could plausibly happen due to random chance.\n",
    "\n",
    "Conclusion:\n",
    "In conclusion there's no statistically significant difference in the 'follow_up_metric' between group A and group B based on the A/B test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f31e8",
   "metadata": {},
   "source": [
    "#### Creating a KPI Dashboard\n",
    "\n",
    "#### How to Use\n",
    "The dashboard is set up so that way you can set your x and y on the preferred plot (Scatter or Bar) and also view the prediction of what's selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bc1593e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8052/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9bc4adfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.express as px\n",
    "\n",
    "# Initialize Dash app with a unique name\n",
    "prediction_results_app = dash.Dash(__name__)\n",
    "\n",
    "# App layout\n",
    "prediction_results_app.layout = html.Div([\n",
    "    html.H1(\"US Foods - Survey Data Assessment: Prediction Results Dashboard\"),\n",
    "\n",
    "    # Dropdown for selecting Plot Type\n",
    "    dcc.Dropdown(\n",
    "        id='plot-type-dropdown',\n",
    "        options=[{'label': 'Scatter Plot', 'value': 'scatter'}, {'label': 'Bar Graph', 'value': 'bar'}],\n",
    "        value='scatter',  # Default value\n",
    "        style={'width': '48%'}\n",
    "    ),\n",
    "\n",
    "    # Dropdown for selecting X component\n",
    "    dcc.Dropdown(\n",
    "        id='x-axis-dropdown',\n",
    "        options=[{'label': feature, 'value': feature} for feature in df_downsampled.columns],\n",
    "        value=df_downsampled.columns[0],  # Default value\n",
    "        style={'width': '48%'}\n",
    "    ),\n",
    "\n",
    "    # Dropdown for selecting Y component\n",
    "    dcc.Dropdown(\n",
    "        id='y-axis-dropdown',\n",
    "        options=[{'label': feature, 'value': feature} for feature in df_downsampled.columns],\n",
    "        value=df_downsampled.columns[1],  # Default value\n",
    "        style={'width': '48%'}\n",
    "    ),\n",
    "\n",
    "    # Dropdown for selecting Predictor (either 'target' or 'y_pred')\n",
    "    dcc.Dropdown(\n",
    "        id='predictor-dropdown',\n",
    "        options=[{'label': predictor, 'value': predictor} for predictor in ['target', 'y_pred']],\n",
    "        value='target',  # Default value\n",
    "        style={'width': '48%'}\n",
    "    ),\n",
    "\n",
    "    # Graph Plot\n",
    "    dcc.Graph(id='plot')\n",
    "])\n",
    "\n",
    "# Callback to update plot\n",
    "@prediction_results_app.callback(\n",
    "    Output('plot', 'figure'),\n",
    "    [\n",
    "        Input('plot-type-dropdown', 'value'),\n",
    "        Input('x-axis-dropdown', 'value'),\n",
    "        Input('y-axis-dropdown', 'value'),\n",
    "        Input('predictor-dropdown', 'value')\n",
    "    ]\n",
    ")\n",
    "def update_plot(plot_type, x_axis, y_axis, predictor):\n",
    "    if plot_type == 'scatter':\n",
    "        fig = px.scatter(df_downsampled, x=x_axis, y=y_axis, color=predictor)\n",
    "    elif plot_type == 'bar':\n",
    "        fig = px.histogram(df_downsampled, x=x_axis, color=predictor, barmode='overlay')\n",
    "    return fig\n",
    "\n",
    "# Run the app on a different port to avoid conflicts\n",
    "if __name__ == '__main__':\n",
    "    prediction_results_app.run_server(debug=True, port=8052)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce154ef7",
   "metadata": {},
   "source": [
    "##### Solving Scenario 1: Predicting Customer Spending with US Foods\n",
    "\n",
    "The main goal is to predict the weekly spending a customer does with US Foods based on various influencing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbef200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 8432693.163878003\n",
      "R-squared: 0.6763147750005258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split data\n",
    "X = df_downsampled[[\"REPORTED_WKLY_SPEND\", \"SMALL_QTY_RANK\", \"DEL_FLEX_RANK\", \"CUT_TIME_RANK\", \"WKLY_ORDERS\", \"PERC_EB\", \"MENU_TYP_DESC\", \"PYR_SEG_CD\", \"AVG_WKLY_SALES\", \"AVG_WKLY_CASES\"]]\n",
    "y = df_downsampled[\"REPORTED_WKLY_SPEND_USF\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = [\"REPORTED_WKLY_SPEND\", \"SMALL_QTY_RANK\", \"DEL_FLEX_RANK\", \"CUT_TIME_RANK\", \"WKLY_ORDERS\", \"PERC_EB\", \"AVG_WKLY_SALES\", \"AVG_WKLY_CASES\"]\n",
    "categorical_features = [\"MENU_TYP_DESC\", \"PYR_SEG_CD\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # Added handle_unknown='ignore'\n",
    "    ])\n",
    "\n",
    "# Create and evaluate the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'R-squared: {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b51710",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "MSE (Mean Squared Error): Represents the average of the squares of the errors or deviations, i.e., the difference between the estimator and what is estimated. A lower MSE indicates a better fit of the model to the data.\n",
    "\n",
    "R-Squared: 67.63% means that the model's predictions are about 67.63% closer to the real values compared to if we just guessed using the average value every single time. A higher R-Squared is generally better, with 100% meaning your model captures everything perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72067a44",
   "metadata": {},
   "source": [
    "Based on these results. We can now try some other types of modeling to see if it's a better fit to represent the data. \n",
    "1. Identify feature importance using the coefficients from the Linear Regression model (applicable for linear regression).\n",
    "2. Optimize the model by trying Ridge Regression, which can handle multicollinearity better than simple linear regression.\n",
    "3. Evaluate the model with cross-validation to get a more robust performance measure.\n",
    "\n",
    "To further explain why I'm doing what I'm doing, you can follow what I am writing here and observe my approach programmatically. \n",
    "\n",
    "###### Ridge Regression with Cross-Validation (RidgeCV): \n",
    "We're using Ridge Regression along with its in-built cross-validation method. The alphas parameter represents different levels of regularization strength. Regularization can prevent overfitting and handle multicollinearity better.\n",
    "\n",
    "###### Cross-Validation on the Entire Dataset: \n",
    "We use cross_val_score to perform 5-fold cross-validation on the entire dataset to get a better sense of how well our model will generalize to unseen data.\n",
    "\n",
    "###### Feature Importance: \n",
    "For linear models like Ridge Regression, the magnitude of the coefficients can be used as a measure of feature importance. We extract these coefficients and display them sorted by importance. OneHotEncoder has transformed categorical features into multiple columns, so we retrieve those new column names to display feature importance accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "964342f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated R-squared scores: [ 5.62003336e-01  4.88805843e-01  6.32723778e-01 -3.43918173e+03\n",
      " -4.23698347e+04]\n",
      "Average R-squared score: -9161.47\n",
      "Standard Deviation of R-squared score: 16657.54\n",
      "\n",
      "Feature Importances:\n",
      "WKLY_ORDERS               1294.81\n",
      "REPORTED_WKLY_SPEND       601.66\n",
      "SMALL_QTY_RANK            272.35\n",
      "AVG_WKLY_SALES            214.22\n",
      "MENU_TYP_DESC_MEXICAN     186.18\n",
      "MENU_TYP_DESC_BAR & GRILL 111.81\n",
      "AVG_WKLY_CASES            96.41\n",
      "PYR_SEG_CD_Education      84.96\n",
      "MENU_TYP_DESC_ITALIAN- PIZZA & PASTA 79.63\n",
      "PYR_SEG_CD_Regional       71.72\n",
      "MENU_TYP_DESC_EUROPEAN    69.75\n",
      "MENU_TYP_DESC_FUSION / ECLECTIC INTERNATIONAL 46.89\n",
      "MENU_TYP_DESC_BARBECUE    43.24\n",
      "PYR_SEG_CD_Retail         42.68\n",
      "DEL_FLEX_RANK             32.98\n",
      "MENU_TYP_DESC_VARIED MENU 15.30\n",
      "MENU_TYP_DESC_BRAZILIAN   15.16\n",
      "MENU_TYP_DESC_SOUTHERN & SOUL 8.81\n",
      "PYR_SEG_CD_National Chain 4.07\n",
      "MENU_TYP_DESC_DONUTS      2.91\n",
      "PERC_EB                   0.95\n",
      "MENU_TYP_DESC_HAMBURGERS  -0.66\n",
      "MENU_TYP_DESC_SMOOTHIE / JUICE -0.76\n",
      "MENU_TYP_DESC_FROZEN DESSERTS -2.38\n",
      "MENU_TYP_DESC_HAWAIIAN    -4.96\n",
      "MENU_TYP_DESC_CARIBBEAN   -5.63\n",
      "MENU_TYP_DESC_GREEK       -6.71\n",
      "PYR_SEG_CD_Health Care    -9.63\n",
      "PYR_SEG_CD_Hospitality    -9.98\n",
      "MENU_TYP_DESC_KOREAN      -11.91\n",
      "MENU_TYP_DESC_JAPANESE    -13.10\n",
      "PYR_SEG_CD_Government     -13.77\n",
      "MENU_TYP_DESC_BAKERIES / DONUTS / SNACKS -18.95\n",
      "MENU_TYP_DESC_PIZZERIA    -20.17\n",
      "CUT_TIME_RANK             -24.25\n",
      "MENU_TYP_DESC_AFRICAN     -38.99\n",
      "MENU_TYP_DESC_FRENCH      -40.12\n",
      "MENU_TYP_DESC_OTHER LATIN AMERICA -48.56\n",
      "MENU_TYP_DESC_AMERICAN    -48.59\n",
      "PYR_SEG_CD_Independent Restaurant -55.17\n",
      "MENU_TYP_DESC_SANDWICHES / SOUPS / SALADS -64.21\n",
      "MENU_TYP_DESC_COFFEE / TEA -67.85\n",
      "MENU_TYP_DESC_STEAK & SEAFOOD -83.90\n",
      "MENU_TYP_DESC_MT UNAVAILABLE -102.21\n",
      "PYR_SEG_CD_Other          -114.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create and evaluate the pipeline with Ridge Regression\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10, 100], store_cv_values=True))\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation to evaluate performance\n",
    "cross_val_scores = cross_val_score(ridge_pipeline, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Cross-validated R-squared scores: {cross_val_scores}\")\n",
    "print(f\"Average R-squared score: {cross_val_scores.mean():.2f}\")\n",
    "print(f\"Standard Deviation of R-squared score: {cross_val_scores.std():.2f}\")\n",
    "\n",
    "# Getting feature importance using Ridge Regression coefficients\n",
    "ridge_regressor = ridge_pipeline.named_steps['regressor']\n",
    "\n",
    "# Alternative method to get one-hot-encoded columns\n",
    "one_hot_columns = ridge_pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names(categorical_features)\n",
    "all_features = numeric_features + list(one_hot_columns)\n",
    "\n",
    "feature_importances = ridge_regressor.coef_\n",
    "\n",
    "# Sort the features by importance\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{all_features[idx]:<25} {feature_importances[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76f5fd",
   "metadata": {},
   "source": [
    "##### Results Explained:\n",
    "\n",
    "###### Cross-Validated R-Squared Scores:\n",
    "The model's performance on different segments (folds) of your data varies widely. The first three values are reasonable, but the last two are extremely negative, pulling the average R-squared into a large negative. This indicates that the model performed very poorly on those particular segments. Negative R-squared values signify that the model is worse than a horizontal line fit to the data.\n",
    "\n",
    "###### Feature Importances: \n",
    "This list ranks the importance of each feature when predicting the target variable. The magnitude of the value represents the strength of the relationship (larger values have a stronger influence on the prediction), while the sign (+ or -) indicates the direction of the relationship:\n",
    "\n",
    "Positive values: As the feature increases, the predicted REPORTED_WKLY_SPEND_USF is also expected to increase.\n",
    "\n",
    "Negative values: As the feature increases, the predicted REPORTED_WKLY_SPEND_USF is expected to decrease.\n",
    "\n",
    "Example: WKLY_ORDERS has the highest positive influence, meaning if weekly orders increase, the weekly spending with US Foods is expected to increase. On the other hand, PYR_SEG_CD_Other has a strong negative influence, suggesting that being categorized under this segment might lead to reduced spending with US Foods.\n",
    "\n",
    "Recommendation for the next step is to see if we can do some simple logorithmic transformations of the values. This is to try and get rid of the negatives and make them positive to create a more linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250a7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated R-squared scores (Transformed): [ 4.78591087e-01  4.93603491e-01  4.53417638e-01 -5.82640576e+03\n",
      " -7.17783606e+03]\n",
      "Average R-squared score (Transformed): -2600.56\n",
      "Standard Deviation of R-squared score (Transformed): 3214.15\n",
      "\n",
      "Feature Importances (Transformed):\n",
      "REPORTED_WKLY_SPEND       1369.95\n",
      "WKLY_ORDERS               1329.66\n",
      "MENU_TYP_DESC_MEXICAN     773.11\n",
      "MENU_TYP_DESC_EUROPEAN    486.72\n",
      "PYR_SEG_CD_Education      464.01\n",
      "MENU_TYP_DESC_HAMBURGERS  379.45\n",
      "MENU_TYP_DESC_FUSION / ECLECTIC INTERNATIONAL 362.32\n",
      "MENU_TYP_DESC_BAR & GRILL 258.36\n",
      "MENU_TYP_DESC_DONUTS      246.90\n",
      "MENU_TYP_DESC_STEAK & SEAFOOD 238.11\n",
      "SMALL_QTY_RANK            228.15\n",
      "PYR_SEG_CD_Health Care    158.34\n",
      "PYR_SEG_CD_Hospitality    142.83\n",
      "PYR_SEG_CD_Regional       134.90\n",
      "AVG_WKLY_CASES            108.27\n",
      "MENU_TYP_DESC_BRAZILIAN   64.30\n",
      "MENU_TYP_DESC_BARBECUE    64.17\n",
      "AVG_WKLY_SALES            54.53\n",
      "MENU_TYP_DESC_FROZEN DESSERTS 37.57\n",
      "MENU_TYP_DESC_SOUTHERN & SOUL 22.39\n",
      "MENU_TYP_DESC_ITALIAN- PIZZA & PASTA 20.33\n",
      "PYR_SEG_CD_Government     10.63\n",
      "MENU_TYP_DESC_KOREAN      4.46\n",
      "PYR_SEG_CD_National Chain -4.79\n",
      "PYR_SEG_CD_Other          -8.46\n",
      "MENU_TYP_DESC_GREEK       -17.47\n",
      "MENU_TYP_DESC_HAWAIIAN    -25.62\n",
      "MENU_TYP_DESC_JAPANESE    -31.78\n",
      "MENU_TYP_DESC_AFRICAN     -43.84\n",
      "MENU_TYP_DESC_AMERICAN    -96.52\n",
      "MENU_TYP_DESC_BAKERIES / DONUTS / SNACKS -113.83\n",
      "MENU_TYP_DESC_CARIBBEAN   -127.85\n",
      "MENU_TYP_DESC_SMOOTHIE / JUICE -141.17\n",
      "DEL_FLEX_RANK             -147.57\n",
      "PERC_EB                   -157.57\n",
      "MENU_TYP_DESC_VARIED MENU -174.55\n",
      "PYR_SEG_CD_Retail         -204.56\n",
      "MENU_TYP_DESC_FRENCH      -232.58\n",
      "MENU_TYP_DESC_PIZZERIA    -243.91\n",
      "MENU_TYP_DESC_OTHER LATIN AMERICA -359.63\n",
      "CUT_TIME_RANK             -380.94\n",
      "MENU_TYP_DESC_MT UNAVAILABLE -397.26\n",
      "MENU_TYP_DESC_COFFEE / TEA -409.03\n",
      "MENU_TYP_DESC_SANDWICHES / SOUPS / SALADS -543.18\n",
      "PYR_SEG_CD_Independent Restaurant -692.91\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a copy of the data to ensure the original remains unchanged\n",
    "X_transformed = X.copy()\n",
    "\n",
    "# List of numeric features that we want to transform\n",
    "# We will avoid transforming categorical features, as log-transform doesn't make sense for them\n",
    "features_to_transform = [\"REPORTED_WKLY_SPEND\", \"WKLY_ORDERS\", \"AVG_WKLY_SALES\", \"AVG_WKLY_CASES\"]\n",
    "\n",
    "# Apply log transformation. We'll use log1p which is log(1 + x), to avoid taking log of zero\n",
    "for feature in features_to_transform:\n",
    "    X_transformed[feature] = np.log1p(X_transformed[feature])\n",
    "\n",
    "# Split the transformed data\n",
    "X_train_transformed, X_test_transformed, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the Ridge Regression pipeline again to evaluate the transformed data\n",
    "ridge_pipeline.fit(X_train_transformed, y_train)\n",
    "cross_val_scores_transformed = cross_val_score(ridge_pipeline, X_transformed, y, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Cross-validated R-squared scores (Transformed): {cross_val_scores_transformed}\")\n",
    "print(f\"Average R-squared score (Transformed): {cross_val_scores_transformed.mean():.2f}\")\n",
    "print(f\"Standard Deviation of R-squared score (Transformed): {cross_val_scores_transformed.std():.2f}\")\n",
    "\n",
    "# Getting feature importance using Ridge Regression coefficients\n",
    "ridge_regressor = ridge_pipeline.named_steps['regressor']\n",
    "\n",
    "# Replace the problematic line to retrieve one-hot encoded feature names\n",
    "one_hot_columns = ridge_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names(categorical_features)\n",
    "all_features = numeric_features + list(one_hot_columns)\n",
    "\n",
    "feature_importances_transformed = ridge_regressor.coef_\n",
    "\n",
    "# Sort the features by importance\n",
    "sorted_indices_transformed = feature_importances_transformed.argsort()[::-1]\n",
    "\n",
    "print(\"\\nFeature Importances (Transformed):\")\n",
    "for idx in sorted_indices_transformed:\n",
    "    print(f\"{all_features[idx]:<25} {feature_importances_transformed[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11254816",
   "metadata": {},
   "source": [
    "##### Result Interpretation:\n",
    "\n",
    "The results indicate that the log-transformed features (REPORTED_WKLY_SPEND, WKLY_ORDERS, and AVG_WKLY_CASES) have significant feature importances, which implies that the transformation might have made them more useful for the model.\n",
    "\n",
    "However, the extreme negative values in the cross-validated R-squared scores are a concern. A negative R-squared indicates that the model is performing worse than a simple horizontal line. These extreme values are likely due to overfitting on some specific folds of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d7666",
   "metadata": {},
   "source": [
    "##### Model Recommendations\n",
    "Given the complexity and potential non-linearity in the data, a non-parametric method like Random Forest may be beneficial. \n",
    "\n",
    "It can handle non-linear relationships, interactions between features without explicitly defining them, and can also provide feature importance.\n",
    "\n",
    "It's also less sensitive to outliers and doesn't require normalization of the features.\n",
    "Random Forest has an added advantage of not overfitting easily because it's an ensemble of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "397e3fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated R-squared scores (Random Forest): [ 8.71559123e-01  7.49876381e-01  6.92470168e-01 -2.61826397e+02\n",
      " -1.92569314e+03]\n",
      "Average R-squared score (Random Forest): -437.04\n",
      "Standard Deviation of R-squared score (Random Forest): 751.24\n",
      "\n",
      "Feature Importances (Random Forest):\n",
      "REPORTED_WKLY_SPEND                      0.6560\n",
      "WKLY_ORDERS                              0.2155\n",
      "AVG_WKLY_CASES                           0.0253\n",
      "CUT_TIME_RANK                            0.0202\n",
      "DEL_FLEX_RANK                            0.0161\n",
      "AVG_WKLY_SALES                           0.0152\n",
      "PERC_EB                                  0.0141\n",
      "PYR_SEG_CD_Hospitality                   0.0104\n",
      "SMALL_QTY_RANK                           0.0104\n",
      "MENU_TYP_DESC_VARIED MENU                0.0026\n",
      "PYR_SEG_CD_Independent Restaurant        0.0022\n",
      "PYR_SEG_CD_Education                     0.0022\n",
      "MENU_TYP_DESC_MT UNAVAILABLE             0.0017\n",
      "MENU_TYP_DESC_BAR & GRILL                0.0012\n",
      "PYR_SEG_CD_Retail                        0.0010\n",
      "MENU_TYP_DESC_EUROPEAN                   0.0010\n",
      "MENU_TYP_DESC_STEAK & SEAFOOD            0.0008\n",
      "MENU_TYP_DESC_HAMBURGERS                 0.0007\n",
      "MENU_TYP_DESC_ITALIAN- PIZZA & PASTA     0.0006\n",
      "MENU_TYP_DESC_MEXICAN                    0.0004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define a Random Forest pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "rf_pipeline.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate performance on cross-validation\n",
    "cross_val_scores_rf = cross_val_score(rf_pipeline, X_transformed, y, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Cross-validated R-squared scores (Random Forest): {cross_val_scores_rf}\")\n",
    "print(f\"Average R-squared score (Random Forest): {cross_val_scores_rf.mean():.2f}\")\n",
    "print(f\"Standard Deviation of R-squared score (Random Forest): {cross_val_scores_rf.std():.2f}\")\n",
    "\n",
    "# Feature Importance\n",
    "rf_regressor = rf_pipeline.named_steps['regressor']\n",
    "rf_importances = rf_regressor.feature_importances_\n",
    "\n",
    "# Sort the features by importance\n",
    "sorted_indices_rf = rf_importances.argsort()[::-1]\n",
    "\n",
    "print(\"\\nFeature Importances (Random Forest):\")\n",
    "for idx in sorted_indices_rf[:20]:  # Displaying top 20 for brevity\n",
    "    print(f\"{all_features[idx]:<40} {rf_importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f11890",
   "metadata": {},
   "source": [
    "##### Results Explained:\n",
    "The Random Forest results show a significant improvement over the Ridge regression models. The R-squared values in the positive range indicate that the model is capturing a substantial portion of the variance in the target variable for some of the cross-validation folds. However, the negative R-squared values for some folds are concerning, as they imply the model's predictions are worse than a naive mean-based model for those folds. This large variation in cross-validation scores suggests the presence of certain subsets of data where the model struggles.\n",
    "\n",
    "##### Feature Importances:\n",
    "The Random Forest model provides a clear indication of which features it finds most important:\n",
    "\n",
    "REPORTED_WKLY_SPEND: This is by far the most important feature, which makes intuitive sense as the weekly spend could directly influence the target variable.\n",
    "\n",
    "WKLY_ORDERS: This feature is the second most influential, which also makes sense. The number of weekly orders might correlate with the performance of the business.\n",
    "\n",
    "The other features have significantly less influence, but features like AVG_WKLY_CASES, CUT_TIME_RANK, DEL_FLEX_RANK, etc., also contribute to the predictions.\n",
    "\n",
    "##### What's Next?\n",
    "Let's start by tuning the Random Forest model using a grid search to find the best hyperparameters. Hyperparameter tuning can often enhance the performance of the model significantly.\n",
    "\n",
    "Given the computational requirements of grid search, I will give a broad range of parameters for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00eb6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best Parameters from Grid Search: {'classifier__bootstrap': True, 'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Detect non-numeric columns\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Define transformers in a column transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, non_numeric_cols)\n",
    "    ], \n",
    "    remainder='passthrough'  # let other columns pass through without changes\n",
    ")\n",
    "\n",
    "# Incorporate preprocessor and the Random Forest Regressor into a pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestRegressor())])\n",
    "\n",
    "# Define the parameters for the grid search\n",
    "# Note: We've prefixed the parameter names with 'classifier__' to ensure they're applied to the RandomForestRegressor within the pipeline\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='r2')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters from the grid search\n",
    "print(\"Best Parameters from Grid Search:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator for predictions and further evaluations\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37eb3a7",
   "metadata": {},
   "source": [
    "##### Interpretation of Results\n",
    "bootstrap: True - This indicates that bootstrapping was used when building trees. Bootstrapping means sampling with replacement.\n",
    "\n",
    "max_depth: 20 - Each tree in the forest will have a maximum depth of 20. This ensures that trees do not grow too deep, which can cause overfitting.\n",
    "\n",
    "min_samples_leaf: 2 - A leaf node will require a minimum of 2 samples. This further prevents trees from growing very deep and capturing noise.\n",
    "\n",
    "min_samples_split: 2 - Internal nodes (non-leaf nodes) will split only if they have at least 2 samples.\n",
    "\n",
    "n_estimators: 100 - The number of trees in the forest will be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebb55c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.7170453142416806\n",
      "Mean Squared Error: 7371575.407204034\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics to evaluate performance (e.g., R^2, Mean Squared Error, etc.)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"R^2 Score:\", r2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4a1e2",
   "metadata": {},
   "source": [
    "R-Squared Score: Approximately 70.26% of the variance in the dependent variable is predictable from the independent variables. In other words, the model explains about 70.26% of the variability of the response data around its mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44ae2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 2715.0645309465544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77fe4cb",
   "metadata": {},
   "source": [
    "The model's predictions are off by approximately $2783.30.\n",
    "\n",
    "##### Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e2d9560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c9ef7d6910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.express as px\n",
    "from dash.dependencies import Input, Output\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a combined DataFrame for visualization\n",
    "df = X_train.copy()\n",
    "df['REPORTED_WKLY_SPEND_USF'] = y_train\n",
    "\n",
    "# Convert non-numeric columns where possible\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Drop any remaining non-numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Handle 'MT UNAVAILABLE' and NaN values by replacing them with the median\n",
    "for col in df.columns:\n",
    "    df[col].replace('MT UNAVAILABLE', np.nan, inplace=True)\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Train a linear regression model\n",
    "X = df.drop('REPORTED_WKLY_SPEND_USF', axis=1)\n",
    "y = df['REPORTED_WKLY_SPEND_USF']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "df['PREDICTED_SPEND_USF'] = model.predict(X)\n",
    "df['ERROR'] = df['REPORTED_WKLY_SPEND_USF'] - df['PREDICTED_SPEND_USF']\n",
    "\n",
    "# Initialize Dash App\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP], title=\"US Foods Spending Insights\")\n",
    "\n",
    "# Heatmap and Pair Plots functions\n",
    "def generate_heatmap():\n",
    "    fig = px.imshow(df.corr(), title=\"Correlation Heatmap\")\n",
    "    fig.update_layout(coloraxis_colorbar_title=\"Correlation Value\")\n",
    "    return fig\n",
    "\n",
    "def generate_pair_plots():\n",
    "    fig = px.scatter_matrix(df[['REPORTED_WKLY_SPEND_USF', 'REPORTED_WKLY_SPEND', 'WKLY_ORDERS', 'PERC_EB']],\n",
    "                            dimensions=[\"REPORTED_WKLY_SPEND_USF\", \"REPORTED_WKLY_SPEND\", \"WKLY_ORDERS\", \"PERC_EB\"],\n",
    "                            title=\"Pair Plots\",\n",
    "                            height=1000)\n",
    "    fig.update_traces(diagonal_visible=False)\n",
    "    fig.update_layout(yaxis_tickfont_size=10)\n",
    "    return fig\n",
    "\n",
    "# App layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"US Foods Spending Insights\"),\n",
    "\n",
    "    # Scatter Plot\n",
    "    html.H3(\"Scatter Plots\"),\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='xaxis-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in df.columns],\n",
    "            value='REPORTED_WKLY_SPEND'\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "            id='yaxis-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in df.columns],\n",
    "            value='REPORTED_WKLY_SPEND_USF'\n",
    "        )\n",
    "    ]),\n",
    "    dcc.Graph(id='scatter-plot'),\n",
    "\n",
    "    # Histogram for variable selection\n",
    "    html.H3(\"Histogram of Selected Variable\"),\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='hist-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in df.columns],\n",
    "            value='REPORTED_WKLY_SPEND_USF'\n",
    "        )\n",
    "    ]),\n",
    "    dcc.Graph(id='variable-histogram'),\n",
    "\n",
    "    # Model Performance Metrics\n",
    "    html.H3(\"Model Performance Metrics\"),\n",
    "    html.Div(id='model-metrics'),\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    html.H3(\"Correlation Heatmap\"),\n",
    "    html.P(\"Correlation values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\"),\n",
    "    dcc.Graph(id='heatmap', figure=generate_heatmap()),\n",
    "\n",
    "    # Pair Plots\n",
    "    html.H3(\"Pair Plots (Subset of Data)\"),\n",
    "    dcc.Graph(id='pair-plots', figure=generate_pair_plots())\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('scatter-plot', 'figure'),\n",
    "    [Input('xaxis-dropdown', 'value'),\n",
    "     Input('yaxis-dropdown', 'value')]\n",
    ")\n",
    "def update_scatter_plot(x_selected, y_selected):\n",
    "    return px.scatter(df, x=x_selected, y=y_selected, title=f\"{x_selected} vs {y_selected}\")\n",
    "\n",
    "@app.callback(\n",
    "    Output('variable-histogram', 'figure'),\n",
    "    [Input('hist-dropdown', 'value')]\n",
    ")\n",
    "def update_histogram(selected_column):\n",
    "    return px.histogram(df, x=selected_column, title=f\"Distribution of {selected_column}\")\n",
    "\n",
    "@app.callback(\n",
    "    Output('model-metrics', 'children'),\n",
    "    [Input('xaxis-dropdown', 'value'),\n",
    "     Input('yaxis-dropdown', 'value')]\n",
    ")\n",
    "def update_model_metrics(x, y):\n",
    "    mae = mean_absolute_error(df['REPORTED_WKLY_SPEND_USF'], df['PREDICTED_SPEND_USF'])\n",
    "    r2 = r2_score(df['REPORTED_WKLY_SPEND_USF'], df['PREDICTED_SPEND_USF'])\n",
    "    return [\n",
    "        html.P(f\"Mean Absolute Error: {mae:.2f}\"),\n",
    "        html.P(f\"R-squared: {r2:.2f}\")\n",
    "    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3246e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
